{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "still-geology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Wave data shape :  (50620, 8000)\n",
      "Label data shape :  (50620, 1)\n",
      "✅\n",
      "LABEL :  ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence']\n",
      "Indexed LABEL :  {'yes': 0, 'no': 1, 'up': 2, 'down': 3, 'left': 4, 'right': 5, 'on': 6, 'off': 7, 'stop': 8, 'go': 9, 'unknown': 10, 'silence': 11}\n",
      "✅\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<numpy.lib.npyio.NpzFile at 0x7fa0a05801d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import random\n",
    "\n",
    "def init_gpu():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "                print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been \n",
    "             print(e)\n",
    "\n",
    "init_gpu()\n",
    "\n",
    "data_path = os.getenv(\"HOME\")+'/aiffel/speech_recognition/data/speech_wav_8000.npz'\n",
    "speech_data = np.load(data_path)\n",
    "\n",
    "print(\"Wave data shape : \", speech_data[\"wav_vals\"].shape)\n",
    "print(\"Label data shape : \", speech_data[\"label_vals\"].shape)\n",
    "print(\"✅\")\n",
    "\n",
    "\n",
    "target_list = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "\n",
    "label_value = target_list\n",
    "label_value.append('unknown')\n",
    "label_value.append('silence')\n",
    "\n",
    "print('LABEL : ', label_value)\n",
    "\n",
    "new_label_value = dict()\n",
    "for i, l in enumerate(label_value):\n",
    "    new_label_value[l] = i\n",
    "label_value = new_label_value\n",
    "\n",
    "print('Indexed LABEL : ', new_label_value)\n",
    "\n",
    "\n",
    "temp = []\n",
    "for v in speech_data[\"label_vals\"]:\n",
    "    temp.append(label_value[v[0]])\n",
    "label_data = np.array(temp)\n",
    "\n",
    "\n",
    "def one_hot_label(wav, label):\n",
    "    label = tf.one_hot(label, depth=12)\n",
    "    return wav, label\n",
    "print(\"✅\")\n",
    "\n",
    "\n",
    "def wav2spec(wav, fft_size=258): # spectrogram shape을 맞추기위해서 size 변형\n",
    "    D = np.abs(librosa.stft(wav, n_fft=fft_size))\n",
    "    return D\n",
    "\n",
    "speech_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fewer-norwegian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 126)\n",
      "(130, 126)\n"
     ]
    }
   ],
   "source": [
    "spec = wav2spec(speech_data[\"wav_vals\"][0])\n",
    "print(spec.shape)\n",
    "\n",
    "spec2 = wav2spec(speech_data[\"wav_vals\"][1])\n",
    "print(spec2.shape)\n",
    "\n",
    "del spec, spec2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-terminal",
   "metadata": {},
   "source": [
    "# 데이터 준비하기\n",
    "이번 데이터는 크기가 매우 커서, 2만개만 썼다. 이 부분에서 자꾸 커널에 종료되어서 시간을 많이 끌었다. 그리고 list와 numpy array 간을 어떻게 넘나들지 매우 고민했는데 생각보다 쉬워서 황당했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "solved-pricing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 8000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000, 130, 126)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 먼저 빈 리스트를 준비하고\n",
    "spec_data=[]\n",
    "s_data = speech_data['wav_vals'][:20000]\n",
    "print(s_data.shape)\n",
    "for wav in s_data: \n",
    "    stft = wav2spec(wav)\n",
    "    # 일단 데이터를 담는다. 이 때 객체는 numpy array가 담기게 된다. \n",
    "    spec_data.append(stft)\n",
    "\n",
    "# 둘러싼 리스트를 np.array() function을 사용해서 numpy array로 바꿔준다. \n",
    "    spec_data = np.array(spec_data)\n",
    "spec_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "veterinary-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_label = label_data[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dangerous-moscow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 130, 126)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sr = 8000\n",
    "train_wav, test_wav, train_label, test_label = train_test_split(spec_data, \n",
    "                                                                s_label, \n",
    "                                                                test_size=0.1,\n",
    "                                                                shuffle=True)\n",
    "print(train_wav.shape)\n",
    "train_wav = train_wav.reshape([-1, 130, 126, 1]) # add channel for CNN\n",
    "test_wav = test_wav.reshape([-1,  130, 126, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-banner",
   "metadata": {},
   "source": [
    "`train_wav = train_wav.reshape([-1, 130, 126, 1])` -> 이 부분이 헷갈렸는데, reshape를 할 때 행의 크기를 정확히 주지 않고 생성하겠다는 의미가 되어서, 자동으로 크기를 인식하고 넣어준다. cnn을 적용하려면 채널 수대로 뒤에 컬럼이 하나 더 붙어야 해서 1을 넣는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "computational-diagram",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data :  (18000, 130, 126, 1)\n",
      "train labels :  (18000,)\n",
      "test data :  (2000, 130, 126, 1)\n",
      "test labels :  (2000,)\n",
      "✅\n"
     ]
    }
   ],
   "source": [
    "print(\"train data : \", train_wav.shape)\n",
    "print(\"train labels : \", train_label.shape)\n",
    "print(\"test data : \", test_wav.shape)\n",
    "print(\"test labels : \", test_label.shape)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hydraulic-purple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function one_hot_label at 0x7fa009892290> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <function one_hot_label at 0x7fa009892290> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "<BatchDataset shapes: ((None, 130, 126, 1), (None, 12)), types: (tf.float32, tf.float32)>\n",
      "<BatchDataset shapes: ((None, 130, 126, 1), (None, 12)), types: (tf.float32, tf.float32)>\n",
      "✅\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_epochs = 10\n",
    "\n",
    "# the save point\n",
    "checkpoint_dir = os.getenv('HOME')+'/aiffel/speech_recognition/models/wav'\n",
    "\n",
    "checkpoint_dir\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_wav, train_label))\n",
    "train_dataset = train_dataset.map(one_hot_label)\n",
    "train_dataset = train_dataset.repeat().batch(batch_size=batch_size)\n",
    "print(train_dataset)\n",
    "\n",
    "# for test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_wav, test_label))\n",
    "test_dataset = test_dataset.map(one_hot_label)\n",
    "test_dataset = test_dataset.batch(batch_size=batch_size)\n",
    "print(test_dataset)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caroline-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "del speech_data\n",
    "del spec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "thrown-satin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 130, 126, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 130, 126, 32) 320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 130, 126, 32) 9248        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 65, 63, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 65, 63, 64)   18496       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 65, 63, 64)   36928       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 65, 63, 96)] 0           conv2d_3[0][0]                   \n",
      "                                                                 max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 31, 96)   0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 31, 128)  110720      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 31, 128)  147584      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 31, 128)  147584      conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, 32, 31, 224) 0           conv2d_6[0][0]                   \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 15, 224)  0           tf_op_layer_concat_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 15, 256)  516352      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 15, 256)  590080      conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 15, 256)  590080      conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_2 (TensorFlo [(None, 16, 15, 480) 0           conv2d_9[0][0]                   \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 7, 480)    0           tf_op_layer_concat_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 8, 7, 480)    0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 26880)        0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          6881536     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 256)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 12)           3084        activation[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 9,053,036\n",
      "Trainable params: 9,052,524\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "input_shape = (130,126,1)\n",
    "input_tensor = layers.Input(shape=input_shape)\n",
    "\n",
    "x = layers.Conv2D(32, kernel_size=(3,3), padding='same', activation='relu')(input_tensor)\n",
    "x = layers.Conv2D(32, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "skip_1 = layers.MaxPool2D()(x)\n",
    "\n",
    "x = layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu')(skip_1)\n",
    "x = layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "# concat을 할 때 axis 값을 negative하게 주면 역순으로 데이터를 참조하게 된다.\n",
    "x = tf.concat([x, skip_1], -1)\n",
    "skip_2 = layers.MaxPool2D()(x)\n",
    "\n",
    "x = layers.Conv2D(128, kernel_size=(3,3), padding='same', activation='relu')(skip_2)\n",
    "x = layers.Conv2D(128, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(128, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = tf.concat([x, skip_2], -1)\n",
    "skip_3 = layers.MaxPool2D()(x)\n",
    "\n",
    "x = layers.Conv2D(256, kernel_size=(3,3), padding='same', activation='relu')(skip_3)\n",
    "x = layers.Conv2D(256, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(256, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = tf.concat([x, skip_3], -1)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "\n",
    "output_tensor = layers.Dense(12)(x)\n",
    "\n",
    "model_wav_skip = tf.keras.Model(input_tensor, output_tensor)\n",
    "\n",
    "model_wav_skip.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-development",
   "metadata": {},
   "source": [
    "# 모델 생성하기\n",
    "Conv1D -> Conv2D로 바꾸기 위해서는 필터의 크기만 조절해주면 됬었다. 한번에 1차원, 길이 n의 필터를 보다가 필터를 2차원으로 만들되 크기가 n이면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mysterious-snapshot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "optimizer=tf.keras.optimizers.Adam(1e-4)\n",
    "model_wav_skip.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "             optimizer=optimizer,\n",
    "             metrics=['accuracy'])\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "objective-puppy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.getenv('HOME')+'/aiffel/speech_recognition/models/wav_skip'\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 monitor='val_loss',\n",
    "                                                 mode='auto',\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "single-august",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 130, 126, 1), (None, 12)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "analyzed-lithuania",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 87\n",
      "Train for 562 steps, validate for 62 steps\n",
      "Epoch 1/10\n",
      "561/562 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9828\n",
      "Epoch 00001: val_loss improved from 0.16189 to 0.15659, saving model to /home/aiffel-dj15/aiffel/speech_recognition/models/wav_skip\n",
      "562/562 [==============================] - 26s 47ms/step - loss: 0.0686 - accuracy: 0.9828 - val_loss: 0.1566 - val_accuracy: 0.9486\n",
      "Epoch 2/10\n",
      "560/562 [============================>.] - ETA: 0s - loss: 0.0599 - accuracy: 0.9848\n",
      "Epoch 00002: val_loss did not improve from 0.15659\n",
      "562/562 [==============================] - 26s 46ms/step - loss: 0.0599 - accuracy: 0.9848 - val_loss: 0.1601 - val_accuracy: 0.9496\n",
      "Epoch 3/10\n",
      "560/562 [============================>.] - ETA: 0s - loss: 0.0460 - accuracy: 0.9889\n",
      "Epoch 00003: val_loss did not improve from 0.15659\n",
      "562/562 [==============================] - 26s 46ms/step - loss: 0.0459 - accuracy: 0.9889 - val_loss: 0.1853 - val_accuracy: 0.9446\n",
      "Epoch 4/10\n",
      "560/562 [============================>.] - ETA: 0s - loss: 0.0398 - accuracy: 0.9903\n",
      "Epoch 00004: val_loss improved from 0.15659 to 0.14964, saving model to /home/aiffel-dj15/aiffel/speech_recognition/models/wav_skip\n",
      "562/562 [==============================] - 26s 46ms/step - loss: 0.0399 - accuracy: 0.9903 - val_loss: 0.1496 - val_accuracy: 0.9536\n",
      "Epoch 5/10\n",
      "561/562 [============================>.] - ETA: 0s - loss: 0.0328 - accuracy: 0.9925\n",
      "Epoch 00005: val_loss did not improve from 0.14964\n",
      "562/562 [==============================] - 26s 46ms/step - loss: 0.0328 - accuracy: 0.9925 - val_loss: 0.1510 - val_accuracy: 0.9577\n",
      "Epoch 6/10\n",
      "561/562 [============================>.] - ETA: 0s - loss: 0.0297 - accuracy: 0.9933\n",
      "Epoch 00006: val_loss did not improve from 0.14964\n",
      "562/562 [==============================] - 26s 46ms/step - loss: 0.0297 - accuracy: 0.9933 - val_loss: 0.1777 - val_accuracy: 0.9521\n",
      "Epoch 7/10\n",
      "561/562 [============================>.] - ETA: 0s - loss: 0.0260 - accuracy: 0.9950\n",
      "Epoch 00007: val_loss did not improve from 0.14964\n",
      "562/562 [==============================] - 26s 46ms/step - loss: 0.0260 - accuracy: 0.9951 - val_loss: 0.1927 - val_accuracy: 0.9486\n",
      "Epoch 8/10\n",
      "561/562 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9962\n",
      "Epoch 00008: val_loss did not improve from 0.14964\n",
      "562/562 [==============================] - 26s 46ms/step - loss: 0.0208 - accuracy: 0.9962 - val_loss: 0.1690 - val_accuracy: 0.9521\n",
      "Epoch 9/10\n",
      "561/562 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.9972\n",
      "Epoch 00009: val_loss did not improve from 0.14964\n",
      "562/562 [==============================] - 25s 45ms/step - loss: 0.0163 - accuracy: 0.9972 - val_loss: 0.1721 - val_accuracy: 0.9556\n",
      "Epoch 10/10\n",
      "561/562 [============================>.] - ETA: 0s - loss: 0.0161 - accuracy: 0.9971\n",
      "Epoch 00010: val_loss did not improve from 0.14964\n",
      "562/562 [==============================] - 25s 45ms/step - loss: 0.0161 - accuracy: 0.9971 - val_loss: 0.2496 - val_accuracy: 0.9315\n",
      "✅\n"
     ]
    }
   ],
   "source": [
    "#30분 내외 소요 (메모리 사용량에 주의해 주세요.)\n",
    "history_wav = model_wav_skip.fit(train_dataset, epochs=10,\n",
    "                    steps_per_epoch=len(train_wav) // batch_size,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=len(test_wav) // batch_size,\n",
    "                    callbacks=[cp_callback]\n",
    "                    )\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-refrigerator",
   "metadata": {},
   "source": [
    "# 모델 학습시키기\n",
    "여기서 애를 많이 먹었다. 계속 입력데이터의 차원과 모델이 원하는 데이터의 차원이 달라서 애를 먹었는데, GPU초기화를 해주니까 말끔히 사라졌다. 이게 바로 첫번째 cell에 소리소문없이 있던 gpu 초기화 코드가 하는 일이다. 10회만 학습시켰는데 정확도가 의외로 높게 나왔고, 사실 9회부터는 오버피팅이 진행되었다. \n",
    "\n",
    "# 회고\n",
    "파이썬에 익숙하지 않아서 모델을 짜는 것도 일이었지만, 차원을 맞추고 데이터를 가공하는데에 더 애를 먹었다. 진짜로! 해당 노드에서 얻은 것은 skip connection이 무엇인지에 대해서 알게 되고, Conv1D->conv2D로 바꾸는 과정을 직접 해보면서 데이터의 학습 과정을 따라가는 경험을 해본 것이 의미있었다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
