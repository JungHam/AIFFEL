{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "architectural-thing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['They say get ready for the revolution', \"I think it's time we find some sorta solution\", \"Somebody's caught up in the endless pollution\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-external",
   "metadata": {},
   "source": [
    "# 데이터 전처리 하기\n",
    "문장을 토큰화 할 때 스페이스를 구분자로 쓰기 위해서 다음과 같은 과정을 거친다.  \n",
    "1. 글자를 소문자로 바꿀 것\n",
    "2. 문장 부호, 특수문자 등을 만나면 특수문자 양쪽에 공백을 추가한다\n",
    "3. 공백 패턴(광활한 공백이 존재할 수 있으니까)을 만나면 스페이스 하나로 치환한다.\n",
    "4. a-zA-Z?.!,¿를 제외한 모든 문자(공백을 포함해서)를 스페이스 1개로 치환한다. \n",
    "\n",
    "이 과정을 마친 문장의 양 끝에 `<start>`와 `<end>`를 붙인다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "authorized-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "numerical-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> anywhere you wanna <end>',\n",
       " '<start> everything you wanna <end>',\n",
       " '<start> anywhere you wanna <end>',\n",
       " '<start> everything you wanna <end>',\n",
       " '<start> be like a rocket <end>',\n",
       " '<start> take off , take off <end>',\n",
       " '<start> to hell with stares <end>',\n",
       " '<start> and no one s there <end>',\n",
       " '<start> tonight i m here <end>',\n",
       " '<start> me against the beat <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처리한 문자의 일부분을 출력한다. \n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0 or len(sentence) > 20: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "specialized-simulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 1203    6 ...    0    0    0]\n",
      " [   2  284    6 ...    0    0    0]\n",
      " [   2 1203    6 ...    0    0    0]\n",
      " ...\n",
      " [   2    4   20 ...    0    0    0]\n",
      " [   2    4   20 ...    0    0    0]\n",
      " [   2   18    7 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f0ff26d80d0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rolled-portland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29049, 19)\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 53\n"
     ]
    }
   ],
   "source": [
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-boards",
   "metadata": {},
   "source": [
    "텐서플로우에서 제공하는 Tokenizer 패키지를 사용해서 토큰화를 한다. 가사 중에 랩을 하는 가사도 있을 것 같아서 길이를 15에서 20으로 잡았고, 또 문장부호 양 옆을 스페이스로 감싸기 때문에 문장이 생각한 것 보다 더 길어지기 때문이다.  단어장의 크기는 12000개로 잡았다. 그리고 우리가 넣은 말뭉치에서 사전과 tensor를 생성하게 된다. 후에 tensor의 열의 길이까지 padding으로 맞추어주면 토큰화는 끝이 난다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smart-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-fifty",
   "metadata": {},
   "source": [
    "RNN의 특성상 입력데이터는 `<start>`로 시작해서 끝 텐서가 제거된 형태이고 정답데이터는 `<start>`를 제외한 나머지 텐서가 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "antique-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-technique",
   "metadata": {},
   "source": [
    "다른 학습 과정과 다르게 자기 자신을 피드백하면서 학습하기 때문에 train set과 validation set이 필요하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-retrieval",
   "metadata": {},
   "source": [
    "# 학습을 위한 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "confused-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-affect",
   "metadata": {},
   "source": [
    "RNN을 한 layer로 가지는 학습 모델을 생성한다.\n",
    "1. 단어 사전을 내장하는 레이어\n",
    "2. RNN(LSTM, hidden size 조절 가능)\n",
    "3. RNN(LSTM, hidden size 조절 가능)\n",
    "4. Fully Connected Layer : 사전에 있는 단어 수 만큼 class가 생성된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "derived-palestinian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 18), (256, 18)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sixth-anime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 18), (256, 18)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-craft",
   "metadata": {},
   "source": [
    "그리고 tensor를 Dataset으로 만들어 준다. tensor로 만들어주면 train data와 label 데이터의 튜플이 생긴다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "irish-privacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 18, 12001), dtype=float32, numpy=\n",
       "array([[[ 7.4834716e-05, -2.5072775e-04,  2.8282605e-04, ...,\n",
       "         -1.3653112e-05, -9.5466356e-05, -6.9407804e-05],\n",
       "        [-3.7805046e-04, -3.9011674e-04,  4.3227029e-04, ...,\n",
       "         -2.3899565e-04, -2.2197478e-04, -5.9706035e-05],\n",
       "        [-8.1655948e-04, -2.0167952e-04,  6.2687165e-04, ...,\n",
       "         -4.0162381e-04, -1.7932673e-04, -1.0489424e-04],\n",
       "        ...,\n",
       "        [-3.7380247e-03,  1.0089799e-03, -1.1321041e-03, ...,\n",
       "          7.0576230e-04, -3.8626026e-03,  3.2058265e-03],\n",
       "        [-3.9199637e-03,  9.6139399e-04, -1.2580114e-03, ...,\n",
       "          7.9524872e-04, -3.9784065e-03,  3.3919502e-03],\n",
       "        [-4.0734131e-03,  9.1641530e-04, -1.3717652e-03, ...,\n",
       "          8.7712932e-04, -4.0750895e-03,  3.5487814e-03]],\n",
       "\n",
       "       [[ 7.4834716e-05, -2.5072775e-04,  2.8282605e-04, ...,\n",
       "         -1.3653112e-05, -9.5466356e-05, -6.9407804e-05],\n",
       "        [ 1.2180939e-05, -2.5723330e-04,  4.5652702e-04, ...,\n",
       "         -2.3990005e-04, -1.5897730e-04,  2.6256719e-04],\n",
       "        [ 9.4955256e-05, -2.1297862e-04,  5.9115916e-04, ...,\n",
       "         -1.8426137e-04, -3.5758989e-05,  2.6348990e-04],\n",
       "        ...,\n",
       "        [-3.4218677e-03,  1.0020145e-03, -1.1343373e-03, ...,\n",
       "          9.6456835e-04, -3.7203385e-03,  3.0222894e-03],\n",
       "        [-3.6500059e-03,  9.6459180e-04, -1.2675087e-03, ...,\n",
       "          1.0055387e-03, -3.8799634e-03,  3.2144331e-03],\n",
       "        [-3.8441662e-03,  9.2470640e-04, -1.3865313e-03, ...,\n",
       "          1.0442585e-03, -4.0117935e-03,  3.3801231e-03]],\n",
       "\n",
       "       [[ 7.4834716e-05, -2.5072775e-04,  2.8282605e-04, ...,\n",
       "         -1.3653112e-05, -9.5466356e-05, -6.9407804e-05],\n",
       "        [ 7.7699922e-05, -5.9976598e-04,  3.9948081e-04, ...,\n",
       "         -3.6314104e-05, -2.6383848e-04, -1.6047267e-04],\n",
       "        [-1.3220588e-04, -6.5275753e-04,  6.3760683e-04, ...,\n",
       "          1.1000795e-04, -5.9755374e-04, -3.6557304e-04],\n",
       "        ...,\n",
       "        [-3.9053473e-03,  6.8342307e-04, -1.5144383e-03, ...,\n",
       "          8.6686300e-04, -3.6777938e-03,  3.2383776e-03],\n",
       "        [-4.0556360e-03,  6.7501911e-04, -1.6137875e-03, ...,\n",
       "          9.2647906e-04, -3.8221518e-03,  3.4331547e-03],\n",
       "        [-4.1803736e-03,  6.6465302e-04, -1.6991646e-03, ...,\n",
       "          9.8220899e-04, -3.9448324e-03,  3.5943338e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 7.4834716e-05, -2.5072775e-04,  2.8282605e-04, ...,\n",
       "         -1.3653112e-05, -9.5466356e-05, -6.9407804e-05],\n",
       "        [-5.8276524e-05, -3.4435390e-04,  1.6866923e-04, ...,\n",
       "          2.2713252e-06, -1.4339725e-04, -1.9777728e-04],\n",
       "        [-1.7353514e-04, -3.9326333e-04,  8.7100751e-05, ...,\n",
       "         -7.2995594e-05, -2.8022437e-04, -1.2415746e-05],\n",
       "        ...,\n",
       "        [-4.0050694e-03,  7.6131715e-04, -1.6156840e-03, ...,\n",
       "          8.6965447e-04, -4.0833484e-03,  3.6044409e-03],\n",
       "        [-4.1384692e-03,  7.4205553e-04, -1.6924700e-03, ...,\n",
       "          9.3538390e-04, -4.1907630e-03,  3.7251164e-03],\n",
       "        [-4.2480780e-03,  7.2235393e-04, -1.7611472e-03, ...,\n",
       "          9.9584588e-04, -4.2786193e-03,  3.8227295e-03]],\n",
       "\n",
       "       [[ 7.4834716e-05, -2.5072775e-04,  2.8282605e-04, ...,\n",
       "         -1.3653112e-05, -9.5466356e-05, -6.9407804e-05],\n",
       "        [-2.3179315e-04, -2.1404840e-04,  4.1700579e-04, ...,\n",
       "          6.2625310e-05, -7.3196636e-05, -1.9272996e-04],\n",
       "        [-2.8962299e-04, -2.5058488e-04,  3.5103117e-04, ...,\n",
       "          3.1163701e-05, -1.4773077e-05, -3.0187768e-04],\n",
       "        ...,\n",
       "        [-3.7187075e-03,  9.1537245e-04, -1.2993165e-03, ...,\n",
       "          9.0807601e-04, -3.7647502e-03,  3.4947796e-03],\n",
       "        [-3.9007138e-03,  8.6396188e-04, -1.4234271e-03, ...,\n",
       "          9.7499217e-04, -3.9247824e-03,  3.6625150e-03],\n",
       "        [-4.0536420e-03,  8.1667647e-04, -1.5342257e-03, ...,\n",
       "          1.0347837e-03, -4.0571922e-03,  3.7971879e-03]],\n",
       "\n",
       "       [[ 7.4834716e-05, -2.5072775e-04,  2.8282605e-04, ...,\n",
       "         -1.3653112e-05, -9.5466356e-05, -6.9407804e-05],\n",
       "        [ 1.2180939e-05, -2.5723330e-04,  4.5652702e-04, ...,\n",
       "         -2.3990005e-04, -1.5897730e-04,  2.6256719e-04],\n",
       "        [-8.9751200e-05, -1.8611809e-04,  5.5601657e-04, ...,\n",
       "         -5.5244932e-04, -2.2896897e-04,  8.3270093e-04],\n",
       "        ...,\n",
       "        [-3.4681312e-03,  6.5412361e-04, -1.5056068e-03, ...,\n",
       "          6.5161876e-04, -3.9230580e-03,  3.9709299e-03],\n",
       "        [-3.7041837e-03,  6.8802468e-04, -1.6057150e-03, ...,\n",
       "          7.5929606e-04, -4.0726601e-03,  4.0136878e-03],\n",
       "        [-3.9033936e-03,  7.1101211e-04, -1.6902129e-03, ...,\n",
       "          8.5281237e-04, -4.1947225e-03,  4.0486800e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in train_dataset.take(1): break\n",
    "model(src_sample)\n",
    "#모델의 input data shape를 정해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "continental-double",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method TextGenerator.call of <__main__.TextGenerator object at 0x7f0ff2d6fa90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method TextGenerator.call of <__main__.TextGenerator object at 0x7f0ff2d6fa90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "Train for 90 steps, validate for 22 steps\n",
      "Epoch 1/30\n",
      "90/90 [==============================] - 17s 190ms/step - loss: 2.1391 - val_loss: 1.4610\n",
      "Epoch 2/30\n",
      "90/90 [==============================] - 16s 172ms/step - loss: 1.3313 - val_loss: 1.3080\n",
      "Epoch 3/30\n",
      "90/90 [==============================] - 15s 171ms/step - loss: 1.2626 - val_loss: 1.2794\n",
      "Epoch 4/30\n",
      "90/90 [==============================] - 15s 167ms/step - loss: 1.2220 - val_loss: 1.2440\n",
      "Epoch 5/30\n",
      "90/90 [==============================] - 15s 167ms/step - loss: 1.1800 - val_loss: 1.2178\n",
      "Epoch 6/30\n",
      "90/90 [==============================] - 15s 167ms/step - loss: 1.1448 - val_loss: 1.1964\n",
      "Epoch 7/30\n",
      "90/90 [==============================] - 15s 167ms/step - loss: 1.1124 - val_loss: 1.1769\n",
      "Epoch 8/30\n",
      "90/90 [==============================] - 15s 168ms/step - loss: 1.0836 - val_loss: 1.1590\n",
      "Epoch 9/30\n",
      "90/90 [==============================] - 15s 168ms/step - loss: 1.0554 - val_loss: 1.1435\n",
      "Epoch 10/30\n",
      "90/90 [==============================] - 15s 168ms/step - loss: 1.0304 - val_loss: 1.1274\n",
      "Epoch 11/30\n",
      "90/90 [==============================] - 15s 168ms/step - loss: 1.0054 - val_loss: 1.1168\n",
      "Epoch 12/30\n",
      "90/90 [==============================] - 15s 169ms/step - loss: 0.9815 - val_loss: 1.1025\n",
      "Epoch 13/30\n",
      "90/90 [==============================] - 15s 168ms/step - loss: 0.9573 - val_loss: 1.0930\n",
      "Epoch 14/30\n",
      "90/90 [==============================] - 15s 168ms/step - loss: 0.9352 - val_loss: 1.0780\n",
      "Epoch 15/30\n",
      "90/90 [==============================] - 15s 169ms/step - loss: 0.9120 - val_loss: 1.0682\n",
      "Epoch 16/30\n",
      "90/90 [==============================] - 15s 168ms/step - loss: 0.8916 - val_loss: 1.0586\n",
      "Epoch 17/30\n",
      "90/90 [==============================] - 15s 170ms/step - loss: 0.8716 - val_loss: 1.0516\n",
      "Epoch 18/30\n",
      "90/90 [==============================] - 15s 169ms/step - loss: 0.8528 - val_loss: 1.0438\n",
      "Epoch 19/30\n",
      "90/90 [==============================] - 15s 169ms/step - loss: 0.8353 - val_loss: 1.0358\n",
      "Epoch 20/30\n",
      "90/90 [==============================] - 15s 169ms/step - loss: 0.8184 - val_loss: 1.0313\n",
      "Epoch 21/30\n",
      "90/90 [==============================] - 15s 169ms/step - loss: 0.8026 - val_loss: 1.0229\n",
      "Epoch 22/30\n",
      "90/90 [==============================] - 15s 171ms/step - loss: 0.7867 - val_loss: 1.0177\n",
      "Epoch 23/30\n",
      "90/90 [==============================] - 16s 173ms/step - loss: 0.7732 - val_loss: 1.0116\n",
      "Epoch 24/30\n",
      "90/90 [==============================] - 15s 169ms/step - loss: 0.7596 - val_loss: 1.0144\n",
      "Epoch 25/30\n",
      "90/90 [==============================] - 16s 174ms/step - loss: 0.7466 - val_loss: 1.0106\n",
      "Epoch 26/30\n",
      "90/90 [==============================] - 16s 180ms/step - loss: 0.7345 - val_loss: 1.0063\n",
      "Epoch 27/30\n",
      "90/90 [==============================] - 16s 182ms/step - loss: 0.7231 - val_loss: 1.0056\n",
      "Epoch 28/30\n",
      "90/90 [==============================] - 16s 180ms/step - loss: 0.7120 - val_loss: 0.9978\n",
      "Epoch 29/30\n",
      "90/90 [==============================] - 16s 174ms/step - loss: 0.7017 - val_loss: 0.9987\n",
      "Epoch 30/30\n",
      "90/90 [==============================] - 15s 168ms/step - loss: 0.6922 - val_loss: 0.9933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0ff0120610>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-dependence",
   "metadata": {},
   "source": [
    "그리고 optimizer와 loss 계산 함수를 정해주고 Epoch를 돌린다. 생각해보면 열번만 돌려도 될 것 같다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "talented-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다.\n",
    "\n",
    "result = generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-release",
   "metadata": {},
   "source": [
    "그리고 꼬리에 꼬리를 물면서 모델에서 예측한 다음 단어를 붙여나가는 식으로 문장이 생성되기 때문에, 우리가 정해둔 길이에 도달하거나 문장의 끝을 예측할 때 까지 while문을 돌리면서 단어를 계속 예측한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "interstate-portsmouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> i love you <end> \n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "impressive-covering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> i m a survivor <end> \n"
     ]
    }
   ],
   "source": [
    "result2 = generate_text(model, tokenizer, init_sentence=\"<start> i\", max_len=20)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-brass",
   "metadata": {},
   "source": [
    "# 회고\n",
    "RNN의 원리에 대해서 좀 더 쉽게 알 수 있는 노드였다. 그리고 텐서플로우에서 토큰화를 지원해줘서 이번 과정에서는 쉽게 처리할 수 있었는데, 파이토치에서도 이런 기능을 지원할까 하는 생각이 들었다. 마지막으로 문장을 예측하는 방법에 대해서 필요한 전처리 과정을 알 수 있었다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
